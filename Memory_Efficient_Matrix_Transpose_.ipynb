{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE3viTujv7nz",
        "outputId": "f5598eac-e367-49ef-d950-c103766f7dd7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A kernel for memory efficient matrix transpose using shared memory to ensure data reads and data writes are memory coalesced."
      ],
      "metadata": {
        "id": "IM2-RNGEzMLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile trans.cu\n",
        "#include <stdio.h>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "#define BLOCK_SIZE 16  // Optimized for shared memory access\n",
        "\n",
        "__global__ void matrixTransposeShared(float *d_in, float *d_out, int width, int height) {\n",
        "    __shared__ float tile[BLOCK_SIZE][BLOCK_SIZE + 1];  // Avoid shared memory bank conflicts\n",
        "\n",
        "    int x = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
        "    int y = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    // Load into shared memory (check bounds)\n",
        "    if (x < width && y < height) {\n",
        "        tile[threadIdx.y][threadIdx.x] = d_in[y * width + x];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    // Transpose within shared memory and write back to global memory\n",
        "    int transposed_x = blockIdx.y * BLOCK_SIZE + threadIdx.x;\n",
        "    int transposed_y = blockIdx.x * BLOCK_SIZE + threadIdx.y;\n",
        "\n",
        "    if (transposed_x < height && transposed_y < width) {\n",
        "        d_out[transposed_y * height + transposed_x] = tile[threadIdx.x][threadIdx.y];\n",
        "    }\n",
        "}\n",
        "\n",
        "void matrixTransposeHost(float *h_in, float *h_out, int width, int height) {\n",
        "    float *d_in, *d_out;\n",
        "    size_t size = width * height * sizeof(float);\n",
        "\n",
        "    cudaMalloc((void **)&d_in, size);\n",
        "    cudaMalloc((void **)&d_out, size);\n",
        "    cudaMemcpy(d_in, h_in, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 blockDim(BLOCK_SIZE, BLOCK_SIZE);\n",
        "    dim3 gridDim((width + BLOCK_SIZE - 1) / BLOCK_SIZE, (height + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
        "\n",
        "    matrixTransposeShared<<<gridDim, blockDim>>>(d_in, d_out, width, height);\n",
        "    cudaMemcpy(h_out, d_out, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    cudaFree(d_in);\n",
        "    cudaFree(d_out);\n",
        "}\n",
        "//print first few values of the matrix.\n",
        "void printMatrix(float *matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < 8; i++) {\n",
        "        for (int j = 0; j < 8; j++) {\n",
        "            printf(\"%4.1f \", matrix[i * cols + j]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int width = 1024, height = 1024;\n",
        "    float* h_in = (float*)malloc(width * height * sizeof(float));\n",
        "    float* h_out = (float*)malloc(width * height * sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < height; i++)\n",
        "        for (int j = 0; j < width; j++)\n",
        "            h_in[i * width + j] = i * width + j;\n",
        "\n",
        "    printf(\"Original Matrix:\\n\");\n",
        "    printMatrix(h_in, height, width);\n",
        "\n",
        "    matrixTransposeHost(h_in, h_out, width, height);\n",
        "\n",
        "    printf(\"\\nTransposed Matrix:\\n\");\n",
        "    printMatrix(h_out, width, height);\n",
        "\n",
        "    free(h_in);\n",
        "    free(h_out);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCimcZltzNp5",
        "outputId": "94987a22-1c13-4d51-916c-69f0af2dc0c6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting trans.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -gencode=arch=compute_75,code=sm_75 -o trans trans.cu"
      ],
      "metadata": {
        "id": "nkNod8bKzWt2"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./trans"
      ],
      "metadata": {
        "id": "r2ZCGyEkzaL2",
        "outputId": "191665c5-181e-4ba4-d432-d0bf57e3d0bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Matrix:\n",
            " 0.0  1.0  2.0  3.0  4.0  5.0  6.0  7.0 \n",
            "1024.0 1025.0 1026.0 1027.0 1028.0 1029.0 1030.0 1031.0 \n",
            "2048.0 2049.0 2050.0 2051.0 2052.0 2053.0 2054.0 2055.0 \n",
            "3072.0 3073.0 3074.0 3075.0 3076.0 3077.0 3078.0 3079.0 \n",
            "4096.0 4097.0 4098.0 4099.0 4100.0 4101.0 4102.0 4103.0 \n",
            "5120.0 5121.0 5122.0 5123.0 5124.0 5125.0 5126.0 5127.0 \n",
            "6144.0 6145.0 6146.0 6147.0 6148.0 6149.0 6150.0 6151.0 \n",
            "7168.0 7169.0 7170.0 7171.0 7172.0 7173.0 7174.0 7175.0 \n",
            "==3321== NVPROF is profiling process 3321, command: ./trans\n",
            "\n",
            "Transposed Matrix:\n",
            " 0.0 1024.0 2048.0 3072.0 4096.0 5120.0 6144.0 7168.0 \n",
            " 1.0 1025.0 2049.0 3073.0 4097.0 5121.0 6145.0 7169.0 \n",
            " 2.0 1026.0 2050.0 3074.0 4098.0 5122.0 6146.0 7170.0 \n",
            " 3.0 1027.0 2051.0 3075.0 4099.0 5123.0 6147.0 7171.0 \n",
            " 4.0 1028.0 2052.0 3076.0 4100.0 5124.0 6148.0 7172.0 \n",
            " 5.0 1029.0 2053.0 3077.0 4101.0 5125.0 6149.0 7173.0 \n",
            " 6.0 1030.0 2054.0 3078.0 4102.0 5126.0 6150.0 7174.0 \n",
            " 7.0 1031.0 2055.0 3079.0 4103.0 5127.0 6151.0 7175.0 \n",
            "==3321== Profiling application: ./trans\n",
            "==3321== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   65.79%  1.5748ms         1  1.5748ms  1.5748ms  1.5748ms  [CUDA memcpy DtoH]\n",
            "                   32.13%  769.01us         1  769.01us  769.01us  769.01us  [CUDA memcpy HtoD]\n",
            "                    2.09%  49.983us         1  49.983us  49.983us  49.983us  matrixTransposeShared(float*, float*, int, int)\n",
            "      API calls:   95.24%  88.268ms         2  44.134ms  76.295us  88.192ms  cudaMalloc\n",
            "                    4.12%  3.8150ms         2  1.9075ms  923.12us  2.8919ms  cudaMemcpy\n",
            "                    0.33%  307.03us         2  153.52us  103.37us  203.67us  cudaFree\n",
            "                    0.15%  134.99us       114  1.1840us     102ns  55.038us  cuDeviceGetAttribute\n",
            "                    0.14%  134.06us         1  134.06us  134.06us  134.06us  cudaLaunchKernel\n",
            "                    0.01%  10.650us         1  10.650us  10.650us  10.650us  cuDeviceGetName\n",
            "                    0.01%  5.2510us         1  5.2510us  5.2510us  5.2510us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.6320us         3     544ns     155ns  1.2820us  cuDeviceGetCount\n",
            "                    0.00%     715ns         2     357ns     169ns     546ns  cuDeviceGet\n",
            "                    0.00%     546ns         1     546ns     546ns     546ns  cuModuleGetLoadingMode\n",
            "                    0.00%     483ns         1     483ns     483ns     483ns  cuDeviceTotalMem\n",
            "                    0.00%     203ns         1     203ns     203ns     203ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}