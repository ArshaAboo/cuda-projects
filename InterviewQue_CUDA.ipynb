{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2Dred.cu\n",
        "\n",
        "#include <iostream>         // For standard input/output operations\n",
        "#include <cuda_runtime.h>   // For CUDA runtime API\n",
        "#include <device_launch_parameters.h> // Optional but ensures threadIdx, blockIdx, etc., are defined. mostly included with cuda_runtime.h\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define num_threads 512\n",
        "\n",
        "__global__ void reduce2D(float* src, float* dst, int rows, int cols){\n",
        "\n",
        "  int index = threadIdx.x;\n",
        "\n",
        "//Each thread mapped to an individual row.\n",
        "\n",
        "  for (int i=index; i<rows; i+=num_threads){\n",
        "    float sum= 0.0f;\n",
        "    for (int j=0; j<cols; j++) {\n",
        "      sum+=src[i*cols+j];\n",
        "    }\n",
        "    dst[i]=sum;\n",
        "  }\n",
        "\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int rows=10000, cols=10000;\n",
        "  size_t size= rows*cols*sizeof(float);\n",
        "  float* h_src= new float[rows*cols];\n",
        "  float* h_dst = new float[rows];\n",
        "\n",
        "  for (int i=0; i<rows*cols; i++){\n",
        "    h_src[i] = 1.0f;\n",
        "  }\n",
        "\n",
        "  float* d_src, *d_dst;\n",
        "  cudaMalloc(&d_src,size);\n",
        "  cudaMalloc(&d_dst, rows*sizeof(float));\n",
        "\n",
        "  cudaMemcpy(d_src,h_src,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  reduce2D<<<1,num_threads>>>(d_src,d_dst,rows,cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaMemcpy(h_dst,d_dst, rows*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for (int i=0; i<20; i++){\n",
        "    std::cout << h_dst[i] << \" \";\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Os9fSHelerzY",
        "outputId": "78c916a4-346c-4b2f-a1f4-2d93c92e7783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 2Dred.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvpmnldTrwzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -gencode=arch=compute_75,code=sm_75 -o reduction 2Dred.cu"
      ],
      "metadata": {
        "id": "DYsyRojXoNhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNpH54M_RbAU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "nvprof ./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpzFs_8eoNwP",
        "outputId": "d59a37e5-bea2-45c1-d1f5-191d8085a939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==402== NVPROF is profiling process 402, command: ./reduction\n",
            "10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 ==402== Profiling application: ./reduction\n",
            "==402== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   60.12%  145.78ms         1  145.78ms  145.78ms  145.78ms  reduce2D(float*, float*, int, int)\n",
            "                   39.88%  96.695ms         1  96.695ms  96.695ms  96.695ms  [CUDA memcpy HtoD]\n",
            "                    0.00%  5.5040us         1  5.5040us  5.5040us  5.5040us  [CUDA memcpy DtoH]\n",
            "      API calls:   37.60%  147.97ms         2  73.985ms  117.09us  147.85ms  cudaMalloc\n",
            "                   37.05%  145.79ms         1  145.79ms  145.79ms  145.79ms  cudaDeviceSynchronize\n",
            "                   24.92%  98.072ms         2  49.036ms  80.804us  97.992ms  cudaMemcpy\n",
            "                    0.29%  1.1425ms         1  1.1425ms  1.1425ms  1.1425ms  cuDeviceGetPCIBusId\n",
            "                    0.07%  294.08us         1  294.08us  294.08us  294.08us  cudaLaunchKernel\n",
            "                    0.06%  221.59us       114  1.9430us     146ns  86.098us  cuDeviceGetAttribute\n",
            "                    0.00%  15.297us         1  15.297us  15.297us  15.297us  cuDeviceGetName\n",
            "                    0.00%  2.3050us         3     768ns     280ns  1.7020us  cuDeviceGetCount\n",
            "                    0.00%  1.0190us         2     509ns     206ns     813ns  cuDeviceGet\n",
            "                    0.00%     673ns         1     673ns     673ns     673ns  cuDeviceTotalMem\n",
            "                    0.00%     527ns         1     527ns     527ns     527ns  cuModuleGetLoadingMode\n",
            "                    0.00%     372ns         1     372ns     372ns     372ns  cuDeviceGetUuid\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE3viTujv7nz",
        "outputId": "ff6cfbbb-c8fe-4c68-99d8-462fea6777a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "_gYWlLTGg7VP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d2c6ea-247f-41b2-ee79-991931093de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 22 08:22:27 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   63C    P8             13W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install -y nsight-compute-2025.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEqPWQV69USg",
        "outputId": "21292e77-d302-4725-a552-5f09d6bc2c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  nsight-compute-2025.1.1\n",
            "0 upgraded, 1 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 295 MB of archives.\n",
            "After this operation, 1,195 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  nsight-compute-2025.1.1 2025.1.1.2-1 [295 MB]\n",
            "Fetched 295 MB in 4s (74.5 MB/s)\n",
            "Selecting previously unselected package nsight-compute-2025.1.1.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../nsight-compute-2025.1.1_2025.1.1.2-1_amd64.deb ...\n",
            "Unpacking nsight-compute-2025.1.1 (2025.1.1.2-1) ...\n",
            "Setting up nsight-compute-2025.1.1 (2025.1.1.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKjXJKYY9xfO",
        "outputId": "9a437bd7-8b9d-4589-a00c-86f3f367f158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA (R) Nsight Compute Command Line Profiler\n",
            "Copyright (c) 2018-2024 NVIDIA Corporation\n",
            "Version 2024.2.1.0 (build 34372528) (public-release)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu ./reduction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4idPMZxuPmVX",
        "outputId": "54366d7c-8a7f-4f2d-e07a-b25f12fa22f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 6768 (/content/reduction)\n",
            "==PROF== Profiling \"reduce2D\" - 0: 0%....50%....100% - 9 passes\n",
            "1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ==PROF== Disconnected from process 6768\n",
            "[6768] reduction@127.0.0.1\n",
            "  reduce2D(float *, float *, int, int) (1, 1, 1)x(512, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ----------- ------------\n",
            "    Metric Name             Metric Unit Metric Value\n",
            "    ----------------------- ----------- ------------\n",
            "    DRAM Frequency                  Ghz         5.00\n",
            "    SM Frequency                    Mhz       584.97\n",
            "    Elapsed Cycles                cycle      765,119\n",
            "    Memory Throughput                 %         2.20\n",
            "    DRAM Throughput                   %         2.20\n",
            "    Duration                         ms         1.31\n",
            "    L1/TEX Cache Throughput           %        83.12\n",
            "    L2 Cache Throughput               %         1.13\n",
            "    SM Active Cycles              cycle    19,057.08\n",
            "    Compute (SM) Throughput           %         0.21\n",
            "    ----------------------- ----------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "    Section: Launch Statistics\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Metric Name                          Metric Unit    Metric Value\n",
            "    -------------------------------- --------------- ---------------\n",
            "    Block Size                                                   512\n",
            "    Function Cache Configuration                     CachePreferNone\n",
            "    Grid Size                                                      1\n",
            "    Registers Per Thread             register/thread              42\n",
            "    Shared Memory Configuration Size           Kbyte           32.77\n",
            "    Driver Shared Memory Per Block        byte/block               0\n",
            "    Dynamic Shared Memory Per Block       byte/block               0\n",
            "    Static Shared Memory Per Block        byte/block               0\n",
            "    # SMs                                         SM              40\n",
            "    Threads                                   thread             512\n",
            "    Uses Green Context                                             0\n",
            "    Waves Per SM                                                0.01\n",
            "    -------------------------------- --------------- ---------------\n",
            "\n",
            "    OPT   Est. Speedup: 97.5%                                                                                           \n",
            "          The grid for this launch is configured to execute only 1 blocks, which is less than the GPU's 40              \n",
            "          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      \n",
            "          concurrently with other workloads, consider reducing the block size to have at least one block per            \n",
            "          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    \n",
            "          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            \n",
            "          description for more details on launch configurations.                                                        \n",
            "\n",
            "    Section: Occupancy\n",
            "    ------------------------------- ----------- ------------\n",
            "    Metric Name                     Metric Unit Metric Value\n",
            "    ------------------------------- ----------- ------------\n",
            "    Block Limit SM                        block           16\n",
            "    Block Limit Registers                 block            2\n",
            "    Block Limit Shared Mem                block           16\n",
            "    Block Limit Warps                     block            2\n",
            "    Theoretical Active Warps per SM        warp           32\n",
            "    Theoretical Occupancy                     %          100\n",
            "    Achieved Occupancy                        %        50.06\n",
            "    Achieved Active Warps Per SM           warp        16.02\n",
            "    ------------------------------- ----------- ------------\n",
            "\n",
            "    OPT   Est. Local Speedup: 49.94%                                                                                    \n",
            "          The difference between calculated theoretical (100.0%) and measured achieved occupancy (50.1%) can be the     \n",
            "          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   \n",
            "          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   \n",
            "          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     \n",
            "          optimizing occupancy.                                                                                         \n",
            "\n",
            "    Section: GPU and Memory Workload Distribution\n",
            "    -------------------------- ----------- ------------\n",
            "    Metric Name                Metric Unit Metric Value\n",
            "    -------------------------- ----------- ------------\n",
            "    Average DRAM Active Cycles       cycle      143,704\n",
            "    Total DRAM Elapsed Cycles        cycle   52,331,520\n",
            "    Average L1 Active Cycles         cycle    19,057.08\n",
            "    Total L1 Elapsed Cycles          cycle   30,414,656\n",
            "    Average L2 Active Cycles         cycle   460,651.22\n",
            "    Total L2 Elapsed Cycles          cycle   35,784,000\n",
            "    Average SM Active Cycles         cycle    19,057.08\n",
            "    Total SM Elapsed Cycles          cycle   30,414,656\n",
            "    Average SMSP Active Cycles       cycle    19,022.03\n",
            "    Total SMSP Elapsed Cycles        cycle  121,658,624\n",
            "    -------------------------- ----------- ------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ncu --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdor4VELPqpM",
        "outputId": "3e0986a7-df9d-4266-f2fb-7242c484f588"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: ncu [options] [program] [program-arguments]\n",
            "\n",
            "General Options:\n",
            "  -h [ --help ]                         Print this help message.\n",
            "  -v [ --version ]                      Print the version number.\n",
            "  --mode arg (=launch-and-attach)       Select the mode of interaction with the target application:\n",
            "                                          launch-and-attach\n",
            "                                          (launch and attach for profiling)\n",
            "                                          launch\n",
            "                                          (launch and suspend for later attach)\n",
            "                                          attach\n",
            "                                          (attach to launched application)\n",
            "  -p [ --port ] arg (=49152)            Base port for connecting to target application\n",
            "  --max-connections arg (=64)           Maximum number of ports for connecting to target application\n",
            "  --config-file arg (=1)                Use config.ncu-cfg config file to set parameters. Searches in the current \n",
            "                                        working directory and \"$HOME/.config/NVIDIA Corporation\" directory.\n",
            "  --config-file-path arg                Override the default path for config file.\n",
            "\n",
            "Launch Options:\n",
            "  --check-exit-code arg (=1)            Check the application exit code and print an error if it is different than 0. \n",
            "                                        If set, --replay-mode application will stop after the first pass if the exit \n",
            "                                        code is not 0.\n",
            "  --injection-path-32 arg (=../linux-desktop-glibc_2_11_3-x86)\n",
            "                                        Override the default path for the 32-bit injection libraries.\n",
            "  --injection-path-64 arg               Override the default path for the 64-bit injection libraries.\n",
            "  --preload-library arg                 Prepend a shared library to be loaded by the application before the injection \n",
            "                                        libraries.\n",
            "  --call-stack                          Enable CPU Call Stack collection. By default, the native-type stack will be \n",
            "                                        collected, cf. --call-stack-type.\n",
            "  --call-stack-type arg                 Set the call stack type to be collected. More than one type may be specified. \n",
            "                                        Implies --call-stack.\n",
            "                                          native (default)\n",
            "                                          (collect a regular CPU call stack)\n",
            "                                          python\n",
            "                                          (collect a Python call stack)\n",
            "  --nvtx                                Enable NVTX support.\n",
            "  --support-32bit                       Support profiling processes launched from 32-bit applications.\n",
            "  --target-processes arg (=all)         Select the processes you want to profile:\n",
            "                                          application-only\n",
            "                                          (profile only the application process)\n",
            "                                          all\n",
            "                                          (profile the application and its child processes)\n",
            "  --target-processes-filter arg         Set the comma separated expressions to filter which processes are profiled.\n",
            "                                          <process name> Set the exact process name to include for profiling.\n",
            "                                          regex:<expression> Set the regex to include matching process names for \n",
            "                                        profiling.\n",
            "                                            On shells that recognize regular expression symbols as special characters,\n",
            "                                            the expression needs to be escaped with quotes.\n",
            "                                          exclude:<process name> Set the exact process name to exclude for profiling.\n",
            "                                          exclude-tree:<process name> Set the exact process name to exclude\n",
            "                                            for profiling and further process tracking. None of its child processes\n",
            "                                            will be profiled, even if they match a positive filter.\n",
            "                                        The executable name part of the process will be considered in the match.\n",
            "                                        Processing of filters stops at the first match.\n",
            "                                        If any positive filter is specified, only processes matching a positive filter \n",
            "                                        are profiled.\n",
            "  --null-stdin                          Launch the application with '/dev/null' as its standard input. This avoids \n",
            "                                        applications reading from standard input being stopped by SIGTTIN signals and \n",
            "                                        hanging when running as backgrounded processes.\n",
            "\n",
            "Attach Options:\n",
            "  --hostname arg                        Set hostname / ip address for connection target.\n",
            "\n",
            "Common Profile Options:\n",
            "  --kill arg (=0)                       Terminate the target application when the requested --launch-count was \n",
            "                                        profiled.\n",
            "  --replay-mode arg (=kernel)           Mechanism used for replaying a kernel launch multiple times to collect all \n",
            "                                        requested profiling data:\n",
            "                                          kernel (default)\n",
            "                                          (Replay individual kernel launches transparently\n",
            "                                           during the execution of the application.)\n",
            "                                          application\n",
            "                                          (Relaunch the entire application multiple times.\n",
            "                                           Requires deterministic program execution.)\n",
            "                                          range\n",
            "                                          (Replay ranges of kernel launches and API calls\n",
            "                                           during the execution of the application.)\n",
            "                                          app-range\n",
            "                                          (Profile ranges without API capture by relaunching\n",
            "                                           the entire application multiple times.\n",
            "                                           Requires deterministic program execution.)\n",
            "  --app-replay-buffer arg (=file)       Application replay buffer location:\n",
            "                                          file (default)\n",
            "                                          (Replay pass data is buffered in a temporary file. The report is created \n",
            "                                        after profiling completed.)\n",
            "                                          memory\n",
            "                                          (Replay pass data is buffered in memory, and the report is created while \n",
            "                                        profiling.)\n",
            "  --app-replay-match arg (=grid)        Application replay kernel matching strategy, per process and device:\n",
            "                                          name\n",
            "                                          (Matched by name)\n",
            "                                          grid (default)\n",
            "                                          (Matched by name and grid/block size)\n",
            "                                          all\n",
            "                                          (Matched by name, grid/block size, context id and stream id)\n",
            "  --app-replay-mode arg (=strict)       Application replay kernel matching mode:\n",
            "                                          strict (default):  Requires all kernels to match across all\n",
            "                                                             replay passes.\n",
            "                                          relaxed         :  Produces results only for kernels that could\n",
            "                                                             be matched across replay passes.\n",
            "  --graph-profiling arg (=node)         CUDA graph profiling mode:\n",
            "                                          node (default)\n",
            "                                          (Profile individual kernel nodes)\n",
            "                                          graph\n",
            "                                          (Profile entire graphs)\n",
            "  --range-replay-options arg            Range replay options, separated by comma:\n",
            "                                          enable-greedy-sync\n",
            "                                          (Insert ctx sync for applicable deferred APIs during capture)\n",
            "                                          disable-host-restore\n",
            "                                          (Disable restoring device-written host allocations)\n",
            "  --list-sets                           List all section sets found in the search paths.\n",
            "  --set arg                             Identifier of section set to collect. If not specified, the basic set is \n",
            "                                        collected.\n",
            "  --list-sections                       List all sections found in the search paths.\n",
            "  --section-folder arg                  Search path for section files. Not recursive.\n",
            "  --section-folder-recursive arg        Search path for section files. Recursive.\n",
            "  --section-folder-restore              Restore stock files to the default section folder or the folder specified by \n",
            "                                        --section-folder.\n",
            "  --list-rules                          List all analysis rules found in the search paths.\n",
            "  --apply-rules arg (=1)                Apply analysis rules for collected sections. If --rule is not set, all \n",
            "                                        available rules are applied. Allowed values:\n",
            "                                          on/off\n",
            "                                          yes/no\n",
            "  --rule arg                            Identifier of rule to apply. Enables --apply-rules yes.\n",
            "  --import-source arg (=0)              If available from -lineinfo, correlated CUDA source files are permanently \n",
            "                                        imported into the report. Allowed values:\n",
            "                                          on/off\n",
            "                                          yes/no\n",
            "  --source-folders arg                  Comma separated search paths for correlated CUDA source files to import into \n",
            "                                        the report if --import-source option is enabled. Recursive.\n",
            "  --list-metrics                        List all metrics to be collected based on selected sections.\n",
            "  --query-metrics                       Query available metrics for devices on the system. Use --devices and --chips to\n",
            "                                        filter which devices to query. By default, metrics reported by this option \n",
            "                                        require a suffix to be collected. See --query-metrics-mode for details.\n",
            "  --query-metrics-mode arg (=base)      Set the mode for querying metrics. Implies --query-metrics.\n",
            "                                        Available modes:\n",
            "                                          base (default)\n",
            "                                          (base names for metrics)\n",
            "                                          suffix\n",
            "                                          (suffix names for metrics. Use --metrics to specify the base metrics to \n",
            "                                        query)\n",
            "                                          all\n",
            "                                          (full names for metrics)\n",
            "  --query-metrics-collection arg (=profiling)\n",
            "                                        Set which metric collection kind to query. Implies --query-metrics.\n",
            "                                        Available modes:\n",
            "                                          profiling (default)\n",
            "                                          (query metrics available for profiling)\n",
            "                                          pmsampling\n",
            "                                          (query metrics available for pm sampling)\n",
            "  --list-chips                          List all supported chips that can be used with --chips.\n",
            "  --chips arg                           Specify the chips for querying metrics, separated by comma.\n",
            "  --profile-from-start arg (=1)         Set if application should be profiled from its start. Allowed values:\n",
            "                                          on/off\n",
            "                                          yes/no\n",
            "  --disable-profiler-start-stop         Disable start/stop profiling. When set, cu(da)ProfilerStart/Stop APIs are \n",
            "                                        ignored.\n",
            "  --quiet                               Suppress all profiler output.\n",
            "  --verbose                             Make profiler output more verbose.\n",
            "  --cache-control arg (=all)            Control the behavior of the GPU caches during profiling. Allowed values:\n",
            "                                          all\n",
            "                                          none\n",
            "  --clock-control arg (=base)           Control the behavior of the GPU clocks during profiling. Allowed values:\n",
            "                                          base\n",
            "                                          (Lock GPU clocks to base)\n",
            "                                          none\n",
            "                                          (Don't lock clocks)\n",
            "                                          reset\n",
            "                                          (Reset GPU clocks and exit)\n",
            "\n",
            "Filter Profile Options:\n",
            "  --devices arg                         Specify the devices to enable profiling on, separated by comma. By default all \n",
            "                                        devices are enabled.\n",
            "  --filter-mode arg (=global)           Set the filter mode for kernel launches. Available modes:\n",
            "                                           global (default) : Apply provided launch filters on kernel launches\n",
            "                                                              collectively.\n",
            "                                           per-gpu          : Apply provided launch filters on kernel launches\n",
            "                                                              separately on each device. Effective launch filters\n",
            "                                                              for this mode are --launch-count and --launch-skip.\n",
            "                                           per-launch-config: Apply kernel filters and launch filters on kernel\n",
            "                                                              launches separately for each GPU launch parameter \n",
            "                                                              i.e. Grid Size, Block Size and Shared Memory\n",
            "                                        \n",
            "  --kernel-id arg                       Set the identifier to use for matching the kernel to profile. The identifier is\n",
            "                                        of the format \"context-id:stream-id:[name-operator:]kernel-name:invocation-nr\".\n",
            "                                        Skip entries that shouldn't be matched, e.g. use \"::foobar:2\" to match the \n",
            "                                        second invocation of \"foobar\" in any context or stream. Use \":7:regex:^foo:\" to\n",
            "                                        match any kernel in stream 7 beginning with \"foo\" (according to \n",
            "                                        --kernel-name-base).\n",
            "  -k [ --kernel-name ] arg              Filter the kernel in one of the following ways:\n",
            "                                          <kernel name> Set the kernel name for an exact match.\n",
            "                                          regex:<expression> Set the regex to use for matching the kernel name.\n",
            "  --kernel-name-base arg (=function)    Set the basis for --kernel-name, --kernel-id and kernel-name:\n",
            "                                          function\n",
            "                                          demangled\n",
            "                                          mangled\n",
            "  --rename-kernels arg (=1)             Perform simplification on the kernel demangled names.\n",
            "                                        Rename demangled names using a config file. By default, searches for \n",
            "                                        ncu-kernel-renames.yaml config file in the current working directory and \n",
            "                                        \"$HOME/.config/NVIDIA Corporation\" directory. Use --rename-kernels-export \n",
            "                                        option to export the simplified demangled names to the config file.\n",
            "  --rename-kernels-export arg (=0)      Export renamed or simplified kernel demangled names to the config file.\n",
            "                                        By default, exports the ncu-kernel-renames.yaml config file in \n",
            "                                        \"$HOME/.config/NVIDIA Corporation\" directory.\n",
            "  --rename-kernels-path arg             Override the default path to kernel demangled names config file. Only valid \n",
            "                                        while renaming or exporting the kernel demangled names.\n",
            "  -c [ --launch-count ] arg             Limit the number of collected profile results. The count is only incremented \n",
            "                                        for launches that match the kernel filters.\n",
            "  -s [ --launch-skip ] arg (=0)         Set the number of kernel launches to skip before starting to profile. The count\n",
            "                                        is incremented for launches that match the kernel filters only.\n",
            "  --launch-skip-before-match arg (=0)   Set the number of kernel launches to skip before starting to profile. The count\n",
            "                                        is incremented for all launches.\n",
            "  --section arg                         Collect the section by providing section identifier in one of the following \n",
            "                                        ways:\n",
            "                                          <section identifier> Set the section identifier for an exact match.\n",
            "                                          regex:<expression> Set the regex to use for matching the section identifier.\n",
            "                                        If option is not specified, the default section set is collected.\n",
            "                                        Section metrics that cannot be collected will usually generate a warning only.\n",
            "  --metrics arg                         Specify all metrics to be profiled, separated by comma.\n",
            "                                        Names passed to this option support the following prefixes:\n",
            "                                          regex:<expression> Expands to all metrics that partially match the\n",
            "                                                             expression. Enclose the regular expression in\n",
            "                                                             ^...$ to force a full match.\n",
            "                                          group:<name>       Lists all metrics of the metric group with that\n",
            "                                                             name. See section files for valid group names.\n",
            "                                          breakdown:<metric> Expands to the input metrics of the high-level\n",
            "                                                             throughput metric. If the specified metric does\n",
            "                                                             not support a breakdown, no metrics are added.\n",
            "                                        If a metric requires a suffix to be valid, and no prefix is used this option \n",
            "                                        automatically expands the name to all available first-level sub-metrics.\n",
            "                                        Metrics that cannot be collected will generate an error.\n",
            "  --disable-extra-suffixes              Disables the collection of extra suffixes (avg, min, max, sum). Only collect \n",
            "                                        what is explicity specified.\n",
            "  --nvtx-include arg                    Adds include statement to the NVTX filter, which allows selecting kernels to \n",
            "                                        profile based on NVTX ranges.\n",
            "  --nvtx-exclude arg                    Adds exclude statement to the NVTX filter, which allows selecting kernels to \n",
            "                                        profile based on NVTX ranges.\n",
            "  --range-filter arg                    Filter to profile specified instance(s) of matching NVTX ranges or start/stop \n",
            "                                        ranges created through cu(da)ProfilerStart/Stop APIs\n",
            "                                        Specify in format <yes/no/on/off>:<start/stop range instance(s)>:<NVTX range \n",
            "                                        instance(s)>\n",
            "                                           <yes/no/on/off> : default is 'no/off'. If set to 'yes/on' then NVTX range \n",
            "                                        numbering starts from 1 inside every start/stop range.\n",
            "                                           Numbers can be provided in regex form e.g, [2-4] or 2|3|4 to profile 2nd, \n",
            "                                        3rd and 4th instance of the matching range.\n",
            "                                           NVTX range numbers will be counted for matching range provided using \n",
            "                                        --nvtx-include.\n",
            "\n",
            "PM Sampling Options:\n",
            "  --pm-sampling-interval arg (=0)       Set the PM sampling interval in cycles or ns (depending on the architecture), \n",
            "                                        or determine dynamically when 0.\n",
            "  --pm-sampling-buffer-size arg (=0)    Set the size of the device-sided allocation for PM sampling in bytes, or \n",
            "                                        determine dynamically when 0.\n",
            "  --pm-sampling-max-passes arg (=0)     Set the maximum number of passes used for PM sampling, or determine dynamically\n",
            "                                        when 0.\n",
            "\n",
            "Warp State Sampling Options:\n",
            "  --warp-sampling-interval arg (=auto)  Set the warp state sampling period in the range of [0..31]. Actual frequency is\n",
            "                                        2 ^ (5 + value) cycles. If set to 'auto', the profiler tries to automatically \n",
            "                                        determine a high sampling frequency without skipping samples or overflowing the\n",
            "                                        output buffer.\n",
            "  --warp-sampling-max-passes arg (=5)   Set maximum number of passes used for warp state sampling.\n",
            "  --warp-sampling-buffer-size arg (=33554432)\n",
            "                                        Set the size of the device-sided allocation for warp state samples in bytes.\n",
            "\n",
            "File Options:\n",
            "  --log-file arg                        Send all tool output to the specified file, or\n",
            "                                          one of the standard channels. The file will be overwritten.\n",
            "                                          If the file doesn't exist, a new one will be created.\n",
            "                                          \"stdout\" as the whole file name indicates standard output\n",
            "                                             channel (stdout). (default)\n",
            "                                          \"stderr\" as the whole file name indicates standard error\n",
            "                                             channel (stderr).\n",
            "  -o [ --export ] arg                   Set the output file for writing the profile results. If not set, a temporary \n",
            "                                        file will be used which is removed afterwards.\n",
            "  -f [ --force-overwrite ]              Force overwriting all output, section or config files (any existing files will \n",
            "                                        be overwritten). \n",
            "  -i [ --import ] arg                   Set the input file for reading profile results.\n",
            "  --open-in-ui                          Open report in UI instead of showing result on terminal.\n",
            "\n",
            "Console Output Options:\n",
            "  --csv                                 Use comma-separated values in the output. Implies --print-units base by \n",
            "                                        default.\n",
            "  --page arg (=details)                 Select report page to output:\n",
            "                                          details: sections and rules\n",
            "                                          raw: all collected metrics\n",
            "                                          source: source code\n",
            "                                          session: session and device attributes\n",
            "  --print-source arg                    Select source view:\n",
            "                                          sass\n",
            "                                          ptx\n",
            "                                          cuda\n",
            "                                          cuda,sass\n",
            "                                         Metric correlation with source is available in sass, and cuda,sass source \n",
            "                                        view.\n",
            "                                         Metrics specified with --metrics and specified section file with --section are\n",
            "                                        correlated.\n",
            "                                         Consider restricting the number of selected metrics such that values fit into \n",
            "                                        a single output row.\n",
            "  --resolve-source-file arg             Set a comma separated list of file paths to consider when resolving files for \n",
            "                                        the source output.\n",
            "  --print-details arg (=header)         Select which part of a section should be shown in the details page output:\n",
            "                                          header (default)\n",
            "                                          (Show all metrics from header of the section)\n",
            "                                          body\n",
            "                                          (Show all metrics from body of the section)\n",
            "                                          all\n",
            "                                          (Show all metrics from the section)\n",
            "  --print-metric-name arg (=label)      Select one of the option to show it in the Metric Name column:\n",
            "                                          label (default)\n",
            "                                          (Show metric label)\n",
            "                                          name\n",
            "                                          (Show metric name)\n",
            "                                          label-name\n",
            "                                          (Show both metric label and metric name)\n",
            "  --print-units arg (=auto)             Set scaling of metric units. Allowed values:\n",
            "                                          auto (default)\n",
            "                                          (Scale metrics to fitting magnitude)\n",
            "                                          base\n",
            "                                          (Show metrics with their base unit)\n",
            "  --print-fp                            Show all numeric metrics as floating point numbers.\n",
            "  --print-kernel-base arg (=demangled)  Set the basis for kernel name output. See --kernel-name-base for options.\n",
            "  --print-metric-instances arg (=none)  Set output mode for metrics with instance values:\n",
            "                                          none (default)\n",
            "                                          (Only show aggregate value)\n",
            "                                          values\n",
            "                                          (Show aggregate followed by all instance values)\n",
            "                                          details\n",
            "                                          (Show aggregate value, followed by correlation IDs and instance values)\n",
            "  --print-nvtx-rename arg (=none)       Select how NVTX should be used for renaming:\n",
            "                                          none (default)\n",
            "                                          (Don't use NVTX for renaming)\n",
            "                                          kernel\n",
            "                                          (Rename kernels with the most recent enclosing NVTX push/pop range)\n",
            "  --print-rule-details                  Print additional details for rule results, such as the triggering metrics. \n",
            "                                        Currently has no effect in CSV mode.\n",
            "  --print-summary arg (=none)           Set the summary output mode:\n",
            "                                          none\n",
            "                                          per-gpu\n",
            "                                          per-kernel\n",
            "                                          per-nvtx.\n",
            "\n",
            "Use the --mode switch to select how to use the tool:\n",
            "  Launch and profile a Cuda application:\n",
            "      ncu CuVectorAdd\n",
            "\n",
            "  Launch an application for later attach:\n",
            "      ncu --mode=launch MyApp\n",
            "  Attach to a previously launched application:\n",
            "      ncu --mode=attach --hostname 127.0.0.1\n",
            "  Applications can also be launched or attached-to with the graphical user interface.\n",
            "\n",
            "Select specific launches for profiling:\n",
            "  Profile first two launches of kernel 'foo':\n",
            "      ncu -k foo -c 2 CuVectorAdd\n",
            "\n",
            "Load an existing report:\n",
            "      ncu --import myReport\n",
            "\n",
            "Usage of --nvtx-include and --nvtx-exclude:\n",
            "  ncu --nvtx --nvtx-include \"Domain A@Range A\"\n",
            "     Profile kernels wrapped inside start/end range 'Range A' of 'Domain A'\n",
            "  ncu --nvtx --nvtx-exclude \"Range A]\"\n",
            "    Profile all kernels except kernels wrapped inside push/pop range 'Range A' of <default domain> at the top of the stack.\n",
            "  ncu --nvtx --nvtx-include \"Range A\" --nvtx-exclude \"Range B\"\n",
            "     Profile kernels wrapped inside start/end range 'Range A' but not inside  'Range B' of <default domain>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kM2k3r1Aryx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2Dredmodified.cu\n",
        "\n",
        "#include <iostream>         // For standard input/output operations\n",
        "#include <cuda_runtime.h>   // For CUDA runtime API\n",
        "#include <device_launch_parameters.h> // Optional but ensures threadIdx, blockIdx, etc., are defined. mostly included with cuda_runtime.h\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define num_threads 512\n",
        "\n",
        "__global__ void reduce2Dmod(float* src, float* dst, int rows, int cols){\n",
        "\n",
        "  int index = threadIdx.x;\n",
        "  //Each thread mapped to different elements of each row. (memory coalescing)\n",
        "\n",
        "  for (int i=0; i<rows; i++){\n",
        "    float sum=0.0f;\n",
        "    for (int j=index; j<cols; j+=num_threads){\n",
        "      sum+=src[i*cols+j];\n",
        "    }\n",
        "    __shared__ float partial_sums[num_threads];\n",
        "    partial_sums[index] = sum;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (index==0){\n",
        "    float final=0;\n",
        "    for (int k=0; k<num_threads; k++){\n",
        "      final+=partial_sums[k];\n",
        "    }\n",
        "    dst[i]= final;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int rows=10000, cols=10000;\n",
        "  size_t size= rows*cols*sizeof(float);\n",
        "  float* h_src= new float[rows*cols];\n",
        "  float* h_dst = new float[rows];\n",
        "\n",
        "  for (int i=0; i<rows*cols; i++){\n",
        "    h_src[i] = 1.0f;\n",
        "  }\n",
        "\n",
        "  float* d_src, *d_dst;\n",
        "  cudaMalloc(&d_src,size);\n",
        "  cudaMalloc(&d_dst, rows*sizeof(float));\n",
        "\n",
        "  cudaMemcpy(d_src,h_src,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  reduce2Dmod<<<1,num_threads>>>(d_src,d_dst,rows,cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaMemcpy(h_dst,d_dst, rows*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for (int i=0; i<20; i++){\n",
        "    std::cout << h_dst[i] << \" \";\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d8af2fb-7ff7-4332-ecbf-35a363cceb80",
        "id": "e4flviSxrzPZ"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 2Dredmodified.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc  -gencode=arch=compute_75,code=sm_75 -o 2Dmod 2Dredmodified.cu"
      ],
      "metadata": {
        "id": "5OF7YaVjwq3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./2Dmod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRbpiffDwzY7",
        "outputId": "f9a929f9-175b-451b-e208-dc352bab599a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==6760== NVPROF is profiling process 6760, command: ./2Dmod\n",
            "10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 ==6760== Profiling application: ./2Dmod\n",
            "==6760== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   62.22%  95.397ms         1  95.397ms  95.397ms  95.397ms  [CUDA memcpy HtoD]\n",
            "                   37.78%  57.929ms         1  57.929ms  57.929ms  57.929ms  reduce2Dmod(float*, float*, int, int)\n",
            "                    0.00%  5.6000us         1  5.6000us  5.6000us  5.6000us  [CUDA memcpy DtoH]\n",
            "      API calls:   38.64%  97.039ms         2  48.520ms  83.280us  96.956ms  cudaMalloc\n",
            "                   38.13%  95.774ms         2  47.887ms  105.58us  95.668ms  cudaMemcpy\n",
            "                   23.07%  57.947ms         1  57.947ms  57.947ms  57.947ms  cudaDeviceSynchronize\n",
            "                    0.09%  214.89us         1  214.89us  214.89us  214.89us  cudaLaunchKernel\n",
            "                    0.06%  157.63us       114  1.3820us     120ns  62.115us  cuDeviceGetAttribute\n",
            "                    0.01%  16.030us         1  16.030us  16.030us  16.030us  cuDeviceGetName\n",
            "                    0.00%  6.0000us         1  6.0000us  6.0000us  6.0000us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4550us         3     485ns     137ns  1.0700us  cuDeviceGetCount\n",
            "                    0.00%     922ns         2     461ns     157ns     765ns  cuDeviceGet\n",
            "                    0.00%     797ns         1     797ns     797ns     797ns  cuModuleGetLoadingMode\n",
            "                    0.00%     708ns         1     708ns     708ns     708ns  cuDeviceTotalMem\n",
            "                    0.00%     289ns         1     289ns     289ns     289ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second kernel (my implementation) is better for larger datasets. Execution time of kernel is lesser."
      ],
      "metadata": {
        "id": "sc4MkTQz9hk0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ha7Qkk9J9wj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below is the implementation with two dimensions of threads."
      ],
      "metadata": {
        "id": "iJB6pdF-95EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2Dredmod2D.cu\n",
        "\n",
        "#include <iostream>         // For standard input/output operations\n",
        "#include <cuda_runtime.h>   // For CUDA runtime API\n",
        "#include <device_launch_parameters.h> // Optional but ensures threadIdx, blockIdx, etc., are defined. mostly included with cuda_runtime.h\n",
        "#include <stdlib.h>\n",
        "#include <cmath>\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "#define blocksize 32\n",
        "\n",
        "//Only one block in the x-direction does the actual summation work for each row.\n",
        "//Multiple blocks in the y-direction are used to parallelize across rows.\n",
        "//Only threads in blockIdx.x == 0 are performing all the computation.\n",
        "// Threads in blocks where blockIdx.x > 0 do nothing at all  they are launched but skip the work due to this condition.\n",
        "//(to avoid redundant compute and write to the same dst[i])\n",
        "\n",
        "\n",
        "__global__ void reduce2Dmod(float* src, float* dst, int rows, int cols){\n",
        "\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = threadIdx.x;\n",
        "  int row_stride = gridDim.y * blockDim.y;\n",
        "\n",
        "  __shared__ float partial_sums[blocksize][blocksize];\n",
        "\n",
        "  for (int i=row; i<rows; i+=row_stride){\n",
        "    if (blockIdx.x==0){\n",
        "      float partial=0.0f;\n",
        "      for (int j=col; j<cols; j+=blockDim.x){\n",
        "        partial+=src[i*cols + j];\n",
        "      }\n",
        "\n",
        "      partial_sums[threadIdx.y][threadIdx.x] = partial;\n",
        "\n",
        "      __syncthreads();\n",
        "\n",
        "      if (col==0){\n",
        "        float final=0.0f;\n",
        "        for (int k=0; k<blocksize; k++){\n",
        "           final+=partial_sums[threadIdx.y][k];}\n",
        "\n",
        "\n",
        "        dst[i]=final;\n",
        "      }}\n",
        "    }\n",
        "  }\n",
        "\n",
        "\n",
        "int main(){\n",
        "  int rows=10000, cols=10000;\n",
        "  size_t size= rows*cols*sizeof(float);\n",
        "  float* h_src= new float[rows*cols];\n",
        "  float* h_dst = new float[rows];\n",
        "\n",
        "  for (int i=0; i<rows*cols; i++){\n",
        "    h_src[i] = 1.0f;\n",
        "  }\n",
        "\n",
        "  int minGridSize, blocksize_opt;\n",
        "  cudaOccupancyMaxPotentialBlockSize(&minGridSize, &blocksize_opt,reduce2Dmod , 0, 0);\n",
        "  printf(\"Recommended grid size: %d, block size: %d\\n\", minGridSize, blocksize_opt);\n",
        "\n",
        "  float* d_src, *d_dst;\n",
        "  cudaMalloc(&d_src,size);\n",
        "  cudaMalloc(&d_dst, rows*sizeof(float));\n",
        "\n",
        "  cudaMemcpy(d_src,h_src,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  dim3 blockSize(32,32);\n",
        "  //dim3 gridSize(ceil(rows/blocksize),ceil(cols/blocksize));\n",
        "  dim3 gridSize(1,2);\n",
        "\n",
        "  reduce2Dmod<<<gridSize, blockSize>>>(d_src,d_dst,rows,cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaMemcpy(h_dst,d_dst, rows*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "  float maxerror=0;\n",
        "  for (int i=0; i<rows; i++){\n",
        "    maxerror= max(maxerror,abs(h_dst[i]-rows));\n",
        "  }\n",
        "  cout << maxerror<< endl;\n",
        "\n",
        "  for (int i=0; i<10; i++){\n",
        "    cout << h_dst[i] << \" \";\n",
        "  }\n",
        "\n",
        "\n",
        "  for (int i=rows-1; i>rows-10; i--){\n",
        "    cout << h_dst[i] << \" \";\n",
        "  }\n",
        "  cout << \"hey\" <<endl;\n",
        "  return 0;\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmavJ-W49_AD",
        "outputId": "83a3d3d3-08c6-4a4e-e9e9-946fe7d503c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 2Dredmod2D.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -gencode=arch=compute_75,code=sm_75 -o 2Dmod2D 2Dredmod2D.cu"
      ],
      "metadata": {
        "id": "Z_7fHeZJf28I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./2Dmod2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14VxEuyjgVBK",
        "outputId": "b0fd9628-6dd1-408e-df24-1933c3b3a7e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1302== NVPROF is profiling process 1302, command: ./2Dmod2D\n",
            "Recommended grid size: 40, block size: 1024\n",
            "0\n",
            "10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 hey\n",
            "==1302== Profiling application: ./2Dmod2D\n",
            "==1302== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   74.04%  84.955ms         1  84.955ms  84.955ms  84.955ms  [CUDA memcpy HtoD]\n",
            "                   25.96%  29.783ms         1  29.783ms  29.783ms  29.783ms  reduce2Dmod(float*, float*, int, int)\n",
            "                    0.00%  5.4400us         1  5.4400us  5.4400us  5.4400us  [CUDA memcpy DtoH]\n",
            "      API calls:   65.64%  227.82ms         1  227.82ms  227.82ms  227.82ms  cudaFuncGetAttributes\n",
            "                   24.75%  85.894ms         2  42.947ms  90.906us  85.803ms  cudaMemcpy\n",
            "                    8.59%  29.805ms         1  29.805ms  29.805ms  29.805ms  cudaDeviceSynchronize\n",
            "                    0.34%  1.1675ms         1  1.1675ms  1.1675ms  1.1675ms  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags\n",
            "                    0.33%  1.1282ms         1  1.1282ms  1.1282ms  1.1282ms  cuDeviceGetPCIBusId\n",
            "                    0.30%  1.0252ms         2  512.60us  81.722us  943.47us  cudaMalloc\n",
            "                    0.05%  156.20us       114  1.3700us     116ns  62.292us  cuDeviceGetAttribute\n",
            "                    0.01%  51.196us         1  51.196us  51.196us  51.196us  cudaLaunchKernel\n",
            "                    0.00%  15.212us         1  15.212us  15.212us  15.212us  cuDeviceGetName\n",
            "                    0.00%  3.1650us         1  3.1650us  3.1650us  3.1650us  cudaGetDevice\n",
            "                    0.00%  2.1190us         4     529ns     277ns  1.0230us  cudaDeviceGetAttribute\n",
            "                    0.00%  1.3840us         3     461ns     153ns     986ns  cuDeviceGetCount\n",
            "                    0.00%     784ns         1     784ns     784ns     784ns  cuDeviceTotalMem\n",
            "                    0.00%     762ns         2     381ns     130ns     632ns  cuDeviceGet\n",
            "                    0.00%     678ns         1     678ns     678ns     678ns  cuModuleGetLoadingMode\n",
            "                    0.00%     286ns         1     286ns     286ns     286ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FC1QpokqGSkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below is tree-based reduction"
      ],
      "metadata": {
        "id": "VrVsPqbwlgKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2Dredtree.cu\n",
        "\n",
        "#include <iostream>         // For standard input/output operations\n",
        "#include <cuda_runtime.h>   // For CUDA runtime API\n",
        "#include <device_launch_parameters.h> // Optional but ensures threadIdx, blockIdx, etc., are defined. mostly included with cuda_runtime.h\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define num_threads 512\n",
        "\n",
        "__global__ void reduce2Dmodtree(float* src, float* dst, int rows, int cols){\n",
        "\n",
        "  int index = threadIdx.x;\n",
        "\n",
        "  for (int i=0; i<rows; i++){\n",
        "    float sum=0.0f;\n",
        "    for (int j=index; j<cols; j+=num_threads){\n",
        "      sum+=src[i*cols+j];\n",
        "    }\n",
        "    __shared__ float partial_sums[num_threads];\n",
        "    partial_sums[index] = sum;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "   //tree based reduction -- not just 1 thread is doing the work of summing up all the elements of partial_sum array.\n",
        "   for (int s = 1; s<num_threads; s*=2){\n",
        "    if (index % (s*2)==0){\n",
        "      partial_sums[index]+=partial_sums[index+s];\n",
        "    }\n",
        "    __syncthreads();\n",
        "   }\n",
        "   if (index==0){\n",
        "   dst[i]=partial_sums[0];}\n",
        "\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int rows=10000, cols=10000;\n",
        "  size_t size= rows*cols*sizeof(float);\n",
        "  float* h_src= new float[rows*cols];\n",
        "  float* h_dst = new float[rows];\n",
        "\n",
        "  for (int i=0; i<rows*cols; i++){\n",
        "    h_src[i] = 1.0f;\n",
        "  }\n",
        "\n",
        "  float* d_src, *d_dst;\n",
        "  cudaMalloc(&d_src,size);\n",
        "  cudaMalloc(&d_dst, rows*sizeof(float));\n",
        "\n",
        "  cudaMemcpy(d_src,h_src,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  reduce2Dmodtree<<<1,num_threads>>>(d_src,d_dst,rows,cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaMemcpy(h_dst,d_dst, rows*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for (int i=0; i<20; i++){\n",
        "    std::cout << h_dst[i] << \" \";\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea9602c-ef7a-4a86-e679-eacdfe58b3ba",
        "id": "uNpRe0gyGTRz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 2Dredtree.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -gencode=arch=compute_75,code=sm_75 -o 2Dmodtree 2Dredtree.cu"
      ],
      "metadata": {
        "id": "G8wlz1CqItS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./2Dmodtree"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwWqp3k3IzZ2",
        "outputId": "205f5353-b5f0-4178-e630-45b91262626a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==9872== NVPROF is profiling process 9872, command: ./2Dmodtree\n",
            "10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 ==9872== Profiling application: ./2Dmodtree\n",
            "==9872== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   57.88%  85.340ms         1  85.340ms  85.340ms  85.340ms  [CUDA memcpy HtoD]\n",
            "                   42.11%  62.090ms         1  62.090ms  62.090ms  62.090ms  reduce2Dmodtree(float*, float*, int, int)\n",
            "                    0.00%  5.6320us         1  5.6320us  5.6320us  5.6320us  [CUDA memcpy DtoH]\n",
            "      API calls:   39.85%  98.143ms         2  49.071ms  80.160us  98.063ms  cudaMalloc\n",
            "                   34.77%  85.627ms         2  42.814ms  70.872us  85.557ms  cudaMemcpy\n",
            "                   25.21%  62.096ms         1  62.096ms  62.096ms  62.096ms  cudaDeviceSynchronize\n",
            "                    0.10%  249.90us         1  249.90us  249.90us  249.90us  cudaLaunchKernel\n",
            "                    0.06%  155.09us       114  1.3600us     121ns  66.194us  cuDeviceGetAttribute\n",
            "                    0.00%  12.291us         1  12.291us  12.291us  12.291us  cuDeviceGetName\n",
            "                    0.00%  9.6360us         1  9.6360us  9.6360us  9.6360us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.1730us         3     391ns     125ns     865ns  cuDeviceGetCount\n",
            "                    0.00%     762ns         2     381ns     155ns     607ns  cuDeviceGet\n",
            "                    0.00%     449ns         1     449ns     449ns     449ns  cuModuleGetLoadingMode\n",
            "                    0.00%     431ns         1     431ns     431ns     431ns  cuDeviceTotalMem\n",
            "                    0.00%     288ns         1     288ns     288ns     288ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tn3gtQJCN-Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given below is another implementation of tree-based reduction."
      ],
      "metadata": {
        "id": "1zjY5YQtlogS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2Dredtree1.cu\n",
        "\n",
        "#include <iostream>         // For standard input/output operations\n",
        "#include <cuda_runtime.h>   // For CUDA runtime API\n",
        "#include <device_launch_parameters.h> // Optional but ensures threadIdx, blockIdx, etc., are defined. mostly included with cuda_runtime.h\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define num_threads 512\n",
        "\n",
        "__global__ void reduce2Dmodtree1(float* src, float* dst, int rows, int cols){\n",
        "\n",
        "  int index = threadIdx.x;\n",
        "\n",
        "  for (int i=0; i<rows; i++){\n",
        "    float sum=0.0f;\n",
        "    for (int j=index; j<cols; j+=num_threads){\n",
        "      sum+=src[i*cols+j];\n",
        "    }\n",
        "    __shared__ float partial_sums[num_threads];\n",
        "    partial_sums[index] = sum;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "   //tree based reduction -- not just 1 thread is doing the work of summing up all the elements of partial_sum array.\n",
        "\n",
        "   for (int s=num_threads/2; s>0; s>>=1) {\n",
        "    if (index<s){\n",
        "      partial_sums[index]+=partial_sums[index+s];\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "   }\n",
        "\n",
        "\n",
        "   if (index==0){\n",
        "   dst[i]=partial_sums[0];}\n",
        "\n",
        "  }\n",
        "}\n",
        "\n",
        "int main(){\n",
        "  int rows=10000, cols=10000;\n",
        "  size_t size= rows*cols*sizeof(float);\n",
        "  float* h_src= new float[rows*cols];\n",
        "  float* h_dst = new float[rows];\n",
        "\n",
        "  for (int i=0; i<rows*cols; i++){\n",
        "    h_src[i] = 1.0f;\n",
        "  }\n",
        "\n",
        "  float* d_src, *d_dst;\n",
        "  cudaMalloc(&d_src,size);\n",
        "  cudaMalloc(&d_dst, rows*sizeof(float));\n",
        "\n",
        "  cudaMemcpy(d_src,h_src,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  reduce2Dmodtree1<<<1,num_threads>>>(d_src,d_dst,rows,cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaMemcpy(h_dst,d_dst, rows*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for (int i=0; i<20; i++){\n",
        "    std::cout << h_dst[i] << \" \";\n",
        "  }\n",
        "\n",
        "  return 0;\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c24e9a37-8f69-4af6-b85d-2152c42e2570",
        "id": "M8TLj3kHN-zt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 2Dredtree1.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -gencode=arch=compute_75,code=sm_75 -o 2Dmodtree1 2Dredtree1.cu"
      ],
      "metadata": {
        "id": "Tbec2rwzOnO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./2Dmodtree1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cZ4pRxhPGLR",
        "outputId": "749df61d-c289-4ca0-96ba-4339b6234bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==12007== NVPROF is profiling process 12007, command: ./2Dmodtree1\n",
            "10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 ==12007== Profiling application: ./2Dmodtree1\n",
            "==12007== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   59.62%  91.149ms         1  91.149ms  91.149ms  91.149ms  [CUDA memcpy HtoD]\n",
            "                   40.38%  61.729ms         1  61.729ms  61.729ms  61.729ms  reduce2Dmodtree1(float*, float*, int, int)\n",
            "                    0.00%  5.6320us         1  5.6320us  5.6320us  5.6320us  [CUDA memcpy DtoH]\n",
            "      API calls:   40.95%  106.48ms         2  53.240ms  93.401us  106.39ms  cudaMalloc\n",
            "                   35.17%  91.453ms         2  45.727ms  69.406us  91.384ms  cudaMemcpy\n",
            "                   23.74%  61.738ms         1  61.738ms  61.738ms  61.738ms  cudaDeviceSynchronize\n",
            "                    0.07%  178.26us         1  178.26us  178.26us  178.26us  cudaLaunchKernel\n",
            "                    0.06%  152.15us       114  1.3340us     118ns  59.467us  cuDeviceGetAttribute\n",
            "                    0.01%  13.931us         1  13.931us  13.931us  13.931us  cuDeviceGetName\n",
            "                    0.00%  5.2950us         1  5.2950us  5.2950us  5.2950us  cuDeviceGetPCIBusId\n",
            "                    0.00%  1.4400us         3     480ns     173ns  1.0140us  cuDeviceGetCount\n",
            "                    0.00%  1.0540us         1  1.0540us  1.0540us  1.0540us  cuModuleGetLoadingMode\n",
            "                    0.00%     861ns         2     430ns     120ns     741ns  cuDeviceGet\n",
            "                    0.00%     784ns         1     784ns     784ns     784ns  cuDeviceTotalMem\n",
            "                    0.00%     221ns         1     221ns     221ns     221ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xNgDPnM5dM98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using warp level reductions\n"
      ],
      "metadata": {
        "id": "laKmpXYedRI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 2Dwarp_red.cu\n",
        "\n",
        "#include <iostream>         // For standard input/output operations\n",
        "#include <cuda_runtime.h>   // For CUDA runtime API\n",
        "#include <device_launch_parameters.h> // Optional but ensures threadIdx, blockIdx, etc., are defined. mostly included with cuda_runtime.h\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define num_threads 512\n",
        "\n",
        "__global__ void reduce2Dwarplevel(float* src, float* dst, int rows, int cols){\n",
        "\n",
        "  int index = threadIdx.x;\n",
        "  int warps = num_threads/32;\n",
        "\n",
        "  for (int i=0; i<rows; i++){\n",
        "    float sum=0.0f;\n",
        "    for (int j=index; j<cols; j+=num_threads){\n",
        "      sum+=src[i*cols+j];\n",
        "    }\n",
        "    __shared__ float partial_sums[num_threads/32];\n",
        "\n",
        "    for (int offset = 16; offset > 0; offset /= 2) {\n",
        "    sum += __shfl_down_sync(0xFFFFFFFF, sum, offset);\n",
        "}\n",
        "    int warp_id = index/32;\n",
        "    int lane = index % 32;\n",
        "    if (lane==0) partial_sums[warp_id] = sum;\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    //now we need to do inter-warp reduction.\n",
        "    if (index==0){\n",
        "      float final =0.0f;\n",
        "      for (int k=0; k<warps; k++){\n",
        "        final+=partial_sums[k];\n",
        "      }\n",
        "      dst[i]= final;\n",
        "\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "}}\n",
        "\n",
        "int main(){\n",
        "  int rows=10000, cols=10000;\n",
        "  size_t size= rows*cols*sizeof(float);\n",
        "  float* h_src= new float[rows*cols];\n",
        "  float* h_dst = new float[rows];\n",
        "\n",
        "  for (int i=0; i<rows*cols; i++){\n",
        "    h_src[i] = 1.0f;\n",
        "  }\n",
        "\n",
        "  float* d_src, *d_dst;\n",
        "  cudaMalloc(&d_src,size);\n",
        "  cudaMalloc(&d_dst, rows*sizeof(float));\n",
        "\n",
        "  cudaMemcpy(d_src,h_src,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  reduce2Dwarplevel<<<1,num_threads>>>(d_src,d_dst,rows,cols);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  cudaMemcpy(h_dst,d_dst, rows*sizeof(float),cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for (int i=0; i<20; i++){\n",
        "    std::cout << h_dst[i] << \" \";\n",
        "  }\n",
        "  cudaFree(d_dst);\n",
        "  cudaFree(d_src);\n",
        "  delete[] h_dst;\n",
        "  delete[] h_src;\n",
        "\n",
        "  return 0;\n",
        "\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41109388-e7db-4075-eb87-25b5f07a4687",
        "id": "FNNq6K0JdUjl"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing 2Dwarp_red.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -gencode=arch=compute_75,code=sm_75 -o 2Dmodwarp 2Dwarp_red.cu"
      ],
      "metadata": {
        "id": "iZB2dFWRiAM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvprof ./2Dmodwarp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhPctLxEipcg",
        "outputId": "c2576f74-9c82-4b21-8193-98e8f69290fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==1917== NVPROF is profiling process 1917, command: ./2Dmodwarp\n",
            "10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 10000 ==1917== Profiling application: ./2Dmodwarp\n",
            "==1917== Profiling result:\n",
            "            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
            " GPU activities:   67.57%  94.854ms         1  94.854ms  94.854ms  94.854ms  [CUDA memcpy HtoD]\n",
            "                   32.42%  45.511ms         1  45.511ms  45.511ms  45.511ms  reduce2Dwarplevel(float*, float*, int, int)\n",
            "                    0.00%  5.4730us         1  5.4730us  5.4730us  5.4730us  [CUDA memcpy DtoH]\n",
            "      API calls:   63.41%  249.72ms         2  124.86ms  135.64us  249.58ms  cudaMalloc\n",
            "                   24.43%  96.209ms         2  48.105ms  76.831us  96.133ms  cudaMemcpy\n",
            "                   11.56%  45.520ms         1  45.520ms  45.520ms  45.520ms  cudaDeviceSynchronize\n",
            "                    0.30%  1.1945ms         1  1.1945ms  1.1945ms  1.1945ms  cuDeviceGetPCIBusId\n",
            "                    0.13%  526.62us         2  263.31us  226.18us  300.44us  cudaFree\n",
            "                    0.10%  397.87us       114  3.4900us     106ns  304.59us  cuDeviceGetAttribute\n",
            "                    0.06%  229.50us         1  229.50us  229.50us  229.50us  cudaLaunchKernel\n",
            "                    0.00%  12.305us         1  12.305us  12.305us  12.305us  cuDeviceGetName\n",
            "                    0.00%  1.3610us         3     453ns     146ns     945ns  cuDeviceGetCount\n",
            "                    0.00%     843ns         2     421ns     138ns     705ns  cuDeviceGet\n",
            "                    0.00%     687ns         1     687ns     687ns     687ns  cuDeviceTotalMem\n",
            "                    0.00%     522ns         1     522ns     522ns     522ns  cuModuleGetLoadingMode\n",
            "                    0.00%     285ns         1     285ns     285ns     285ns  cuDeviceGetUuid\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}